Check_MK Hardware+Software Inventur
-----------------------------------

Idee: Check_MK wird ausgebaut zum Inventurisierungssystem. Das ganze passiert wie
mit den Check-Plugins Schritt für Schritt. Aus Sicht des Benutzers stellt sich das
wie folgt dar:

-------------------------------------------------------------------------------
                         F U N K T I O N A L I T Ä T
-------------------------------------------------------------------------------

1. Man Installiert auf seinen Rechnern neue Versionen der Check_MK-Agenten oder
alternativ ein neues Plugin mk_inventory oder sowas.

2. Man löst auf der Kommandozeile cmk --hwinventory HOST oder cmk -W HOST aus.
Alternativ gibt es dafür eine Funktion in WATO. Über eine Regel kann man auch
festlegen, dass dies mit dem Check_MK Inventory-Check automatisch mitläuft.
Oder man macht einen separaten aktiven Check dafür mit einem Interval von
per Default 4 Stunden.

3. Dabei werden von dem Host Daten über Hard- und Software gesammelt und
diese pro Host auf dem Monitoringserver gespeichert. Die Daten sind baumartig
hierarchisch strukturiert und sauber typisiert (Ganzzahl, Fließkommazahl, 
Enumeration aus Liste, etc.).

4. In Multisite gibt es zu jedem Host, der Inventurdaten hat, ein neues
Icon. Damit kommt man zu einer Seite, in der man den Baum eines Hosts
interaktiv aufklappen und ansehen kann.

5. In Multisite gibt es eine neue View, in der man in den Inventurdaten
suchen kann, z.B. alle Hosts, die eine bestimmte Software enthalten,
oder dergleichen.

6. Man kann eine Funktion global konfigurieren, die jedes Update der
Inventurdaten eines Hosts an ein Skript übergibt. Dieses kann die Daten
dann an eine CMBD senden (z.B. i-doit) oder ein Lizenzverwaltungssystem
(z.B. Aspera).




-------------------------------------------------------------------------------
                            U M S E T Z U N G
-------------------------------------------------------------------------------

[1] Agenten

Wir schreiben sowohl für Linux, als auch für Windows ein Plugin mk_hwinventory,
welches Daten sammelt, die von aktuellen Checks nicht ermittelt werden. Ein
Beispiel dafür für Linux:

Code im Plugin:

if which dpkg-query ; then
    echo '<<<deb_packages>>>'
    dpkg-query -W -f='${binary:Package} ${Version}\n'
fi


Die Ausgabe sieht dann so aus:
<<<deb_packages>>>
account-plugin-aim 3.6.0.3-0ubuntu1
account-plugin-facebook 0.8-0ubuntu2.2
account-plugin-flickr 0.8-0ubuntu2.2
account-plugin-google 0.8-0ubuntu2.2
account-plugin-icons 0.8-0ubuntu2.2
account-plugin-identica 0.8-0ubuntu2.2
account-plugin-jabber 3.6.0.3-0ubuntu1
account-plugin-salut 3.6.0.3-0ubuntu1
account-plugin-twitter 0.8-0ubuntu2.2
account-plugin-windows-live 0.8-0ubuntu2.2

Damit die Menge der Daten, die vom Agenten übertragen wird, nicht unnötige
aufgebläht wird, überträgt der Agent die Inventurdaten nur in einem
einstellbaren Turnus. Dazu wird bei Linux die run_cached-Funktion so
erweitert, dass sie eine --once Option bekommt. Diese führt dazu, dass
das Ergebnis eines asynchronen Laufes nur ein einziges Mal ausgeliefert
wird. In den Sektionsheader wird der Schlüssel :persist eingebaut (Beispiel:
<<<deb_packages:persist(1440)>>>). Dieser signalisiert Check_MK, dass diese
Sektion persistiert werden soll. Wenn bei einem Agentenlauf die Sektion fehlt,
wird sie einfach aus diesem Persistenzspeicher geholt. Die Zahl in Klammern
ist die Anzahl von Minuten, für die das gültig sein soll. Wenn innerhalb
dieser Zeit keine neuen Daten kommen, wird das persistierte Verworfen und gilt
als veraltet.  Im aktuellen Windows-Agenten muss das analog umgesetzt werden.

Im Check_mk werden diese Daten abgelegt nach var/check_mk/persist/HOSTNAME/SECTION


[2] Eintrag in die "Datenbank"

Pro Host wird ein logischer Baum angelegt, in dem die Inventurdaten gespeichert
werden. Diese kann von Check_MK aus leicht in eine Python-Struktur eingelesen
werden und wieder gespeichert werden. Die Speicherung erfolgt zunächst
wie üblich als Python-Datenstruktur. Später kann man evtl. eine MongoDB
einsetzen, falls die Recherchefunktionen sonst nicht performant genug sind.

Hier ist ein Beispiel für eine sehr rudimentäre, gekürzte Struktur:
{
    "software": {
        "os" : {
            "brand"        : "windows",
            "product"      : "2008 R2",
            "version"      : "6.1.7601",
            "servicepack"  : "SP1",
            "language"     : "en",
            "architecture" : "x86_64",
        },
        "packager" : "rpm",
        "packages" : [
            ( "l9ibmofoo_bar", "14.3.6b" ),
            ( ... ),
        ],
        "applications" : {
        },
        "filesystems" : {
            "C:\" : {
                "size" : 9600100939,
            },
        },
    },
    "hardware" : {
        "system" : {
            "manufacturer" : "VMware, Inc.",
            "product" : "VMware Virtual Platform",
            "servicetag" : "VMware-423896679ad84101-a2e16c481830174a",
        },
        "cpu" : {
            "cores" : 4,
            "model" : "Intel(R) Core(TM) i7-3930K CPU @ 3.20GH",
        },
        "memory" : {
            "ram" : 10249929400000,
            "swap" : 29384920000000,
        },

        "storage" : {
            "raid_controllers" : [
            ]
        }
    }
}


Im Check_MK-Hauptmodul wird eine Funktion entwickelt, die zunächst die
Agentenausgabe beschafft. Dabei wird der Cache-Verwendet, damit kein
Livezugriff auf den Agenten das Monitoring beinträchtigt. Die Ausgabe
wird wie üblich vorgeparst und dann in ein Dictionary gepackt, welches
alle Sektionen enthält. Bei SNMP-Geräten wird ein analoges Verfahren
gewählt. Dabei können Inventur-Plugins bestimmte OID-Bäume abbonieren,
analog zu den SNMP-basierten Checks.

Nun werden der Reihe nach alle (neu zu schreibenden) Inventur-Plugins
aufgerufen. Diese bekommen als Argumente den zu füllenden Baum und
die oben beschriebene Info. Eine mögliche Funktion könnte so aussehen:

def inventary_foo(info, tree):
    if "deb_packages" in info:
        packages = dict(info["deb_packages"])
        tree["software"]["packager"] = "rpm"
        tree["software"]["packages"] = packages


Es kann auch Plugins geben, die keine Daten aus info auswerten, sondern direkt
in den Baum schauen und weitere Daten daraus ableiten. So könnte man z.B. aus
der RPM-Liste ermitteln, dass auf einem System eine Apache installiert ist.

Die Baumstruktur wird ebenfalls über eine Art Plugins aufgebaut.  Damit wird
sichergestellt, dass der Baum alle Äste enthält, bevor die Inventurplugins
aufgerufen werden. In den Strukturdefinitionen wird auch festgelegt, wie
die einzelnen Äste später dargestellt werden.  Dazu muss man noch ein
Konzept entwickeln.


[3] Hooks und Eintrag in CMDB

Nachdem der Baum für einen Host gefüllt und auf Platte geschrieben wurde,
werden frei definierbare Hookskripten ausgefürt. So ein Hook kann dann in
die CMDB den Update der Daten eines Hosts schreiben. Dazu stellt die CMDB
z.B. einen Webservice bereit, welcher in JSON-Syntax den Baum erhält und
diesen dann komplett oder teilweise in eigene Datenstrukturen ablegt.


[4] Ansicht der Daten in Multisite: Baumdarstellung

In Multisite erstellen wir ein Custom-Icon, welches erkennt, ob zu einem
Host Inventurdaten vorhanden sind. Dieses bringt einen zu einer Seite, in
der die Daten Baumartig dargestellt sind. Das ganze kann analog zu BI
aufbaut sein, mit kleinen Dreiecken, die auf- und zuklappen.


[5] Ansicht der Daten in Multisite: Suchfunktion und Tabellen

Damit man auch ohne CMDB in den Daten recherchieren kann, integrieren wir
eine Ansicht in Multisite. Die Idee ist, dass man nicht eine komplett
eigene Inventur-Ansicht macht, sondern das vorhandene Konzept der Views
nutzt. Dieses ermöglicht heute schon flexibel eigene Ansichten zu definieren
und dabei Sortierung, Gruppierung, Spalten und mehr zu definieren.

Dazu erstellt man Filter und Painter (Spalten) speziell für Inventurdaten.
Das Schöne daran: In Ansichten kann man Filter und Daten aus dem
Monitoring und der Inventurisierung mischen.

Noch ein Problem bei den Spalten: Dinge wie z.B. der Name des Betriebssystems
sind ja einfach anzuzeigen. Wenn aber etwas gebraucht wird wie "Version von
RPM-Paket X", dann ist das X ja variabel. Dies könnten wir evtl. lösen,
in dem wir den bestehenden Mechanismus zum Anzeigen von Service-Daten in
Hosttabellen verwenden.


----------------------------------------------------------------------
Storage
----------------------------------------------------------------------

Storage (EMC, ...), 500 physikalische Platten, daraus dann X logische LUNs.

PCI->Bus, Adapter, Device...

SCSI-Devices (vom FibreChannel-Adapter), z.B. /dev/sdaz, /dev/sdaab
 - Evtl. Partitionen
Multipath-Devices (z.B. Kombination aus sda, sdb, sdc und sdd) --> /dev/dm0
Linux-Software-RAID (z.. /dev/dm0 und /dev/dm1 -> /dev/md0)
 - Spiegelung von Platten
 - Spiegelung von Filern
Das Ding dann z.B. mit kpartx wieder partitioniert!

LVM-PVs z.B. /dev/dm0, kein eigenes Devices
LVM-VGs, bestehen aus mehreren PVs
LVM-LVs, wohnen in einer VG

In den LVs können Dateisysteme wohnen. Oder Swap. Oder ORACLE ASM.

- DR:BD
- Verschlüsselungen
- NFS, SMB, etc.


hardware.storage
    .filesystems (-> Direkt die df-Tabelle)
        :0
            .fsnode     "/dev/vg-01/lv-backup"
            .size       9823749827349823
            .used       2943847238947
            .type       "ext3"
            .mountpoint /srv/backup
            .local      True
            .disk       inv_reference(... zu Platte)
            .partition  inv_reference(... zu Partition)
        :1
            .fsnode     "/dev/vg-01/lv-unused"
            .size       9823749827349823
            .used       2943847238947
            .type       "swap"
            .mountpoint None
            .local      True
        :2
            .server     "rablfals.local"
            .volume     "/wefoijweofijw"
            .size       9823749827349823
            .used       2943847238947
            .type       "nfs"
            .mountpoint "/sapdata"
            .local      False

    .swapareas
        :0
            .device     "/dev/vg-01/lv-swap"
            .size       9823749827349823
            .used       2943847238947

    .devices (LUNs, Harddisks, letztlich alle Blockdevices, außer Partitionen??)
        :1
            .fsnode  "/dev/drbd0"
            .bus     "drbd"  (alternativ: "fibrechannel", "iscsi", "ata")
            .local   False
            .size    24392734983427497
            .vendor  "drbd"
            .partitions
               :0
                    .number 1
                    .size 293847298347
                    .fsnode "/dev/dm5"

    .disks # Jedes Blockdevice, dass faktisch Speicher hinzufügt. LUNs, Platten, DR:BD. Vor RAID
    # Bus-Typen: "ata", "scsi", "fc", "iscsi", "drbd", ???
    # Bei HW-RAID-Platten gibt es (wahrscheinlich) keine fsnode, dafür aber ein
    # .raid -> ID von dem RAID, wo sie verwendet wird
        :0
            .fsnode  "/dev/dm5"
            .alias   "Hirnibaldi"
            .uuid    "3600508b4001073510002c00002820000",
            .bus     "fc"
            .paths   [ "/dev/sda", "/dev/sdb", "/dev/sdc", "/dev/sdd" ],
            .size    24392734983427497
            .wwn    "9239487-23497823-479o-234-982734"
            .local  False
        :1
            .fsnode  "/dev/sda"
            .bus     "ata" (oder "scsi")
            .size    24392734983427497
            .serial  "MK-28348-IF"
            .product "WDC-500cw"
            .vendor  "wdc"
            .type    "disk" ("ssd"??")
            .local   True
            .partitions
               :0
                    .number 2
                    .size 293847298347
                    .fsnode "/dev/sda2"
                    .filesystem inv_reference("hardware.filesystems:2")
               :0
                    .number 5
                    .size 2947298347
                    .swaparea inv_reference("hardware.swapareas:1")
        :2
            .bus     "ata" (oder "scsi")
            .fsnode  "/dev/sdb"
            .size    24392734983427497
            .serial  "MK-28348-IF"
            .product "WDC-500cw"
            .vendor  "wdc"
            .type    "disk" ("ssd"??")
            .raid    0 # Referenz zu hardware.raid:0

    .raid
    # Hardware- oder Software-RAID
        :0
            .size    24392734983427497
            .type    "hw"
            .fsnode  "/dev/cciss/c0d0"
            .harddisks
               # Hier einfach den gleichen Datensatz von .disks nochmal reinhängen
               :0 (symlink: @hardware.disks:17)
               :1

            .partitions
               :0
                    .number 1
                    .size 293847298347
                    .fsnode "/dev/cciss/c0d0p1"


        :1
            .size    24392734983427497
            .type    "sw"
            .fsnode  "/dev/md0"
            .harddisks
               :0
                   .size 9928347923478
                   .fsnode "/dev/sda"
                   .serial "WD-WCAP01993992"
                   .state "online"
               :2
                   .size 9928347923478
                   .fsnode "/dev/sdb"
                   .serial "WD-WCAP01993992"
                   .state "online"

