#!/usr/bin/env python
# -*- encoding: utf-8; py-indent-offset: 4 -*-
# +------------------------------------------------------------------+
# |             ____ _               _        __  __ _  __           |
# |            / ___| |__   ___  ___| | __   |  \/  | |/ /           |
# |           | |   | '_ \ / _ \/ __| |/ /   | |\/| | ' /            |
# |           | |___| | | |  __/ (__|   <    | |  | | . \            |
# |            \____|_| |_|\___|\___|_|\_\___|_|  |_|_|\_\           |
# |                                                                  |
# | Copyright Mathias Kettner 2014             mk@mathias-kettner.de |
# +------------------------------------------------------------------+
#
# This file is part of Check_MK.
# The official homepage is at http://mathias-kettner.de/check_mk.
#
# check_mk is free software;  you can redistribute it and/or modify it
# under the  terms of the  GNU General Public License  as published by
# the Free Software Foundation in version 2.  check_mk is  distributed
# in the hope that it will be useful, but WITHOUT ANY WARRANTY;  with-
# out even the implied warranty of  MERCHANTABILITY  or  FITNESS FOR A
# PARTICULAR PURPOSE. See the  GNU General Public License for more de-
# tails. You should have  received  a copy of the  GNU  General Public
# License along with GNU Make; see the file  COPYING.  If  not,  write
# to the Free Software Foundation, Inc., 51 Franklin St,  Fifth Floor,
# Boston, MA 02110-1301 USA.

VERSION = "1.4.0i1"

import getopt
import fcntl
import fnmatch
import glob
import grp
import io
import os
import pprint
import pwd
import re
import shutil
import socket
import signal
import subprocess
import sys
import textwrap
import time
import threading
import traceback
from tarfile import TarFile
from hashlib import md5

from OpenSSL import crypto
from Crypto.Cipher import AES
from Crypto.PublicKey import RSA
import Crypto.Util.number

import cmk.daemon as daemon
import cmk.render as render
from cmk.exceptions import MKTerminate, MKGeneralException

g_var_path = "/var/lib/mkbackup"

# Is used to duplicate output from stdout/stderr to a the job log. This
# is e.g. used during "mkbackup backup" to store the output.
class Log(object):
    def __init__(self, fd):
        self.fd  = fd

        if self.fd == 1:
            self.orig  = sys.stdout
            sys.stdout = self
        else:
            self.orig  = sys.stderr
            sys.stderr = self

        self.color_replace = re.compile("\033\[\d{1,2}m", re.UNICODE)


    def __del__(self):
        if self.fd == 1:
            sys.stdout = self.orig
        else:
            sys.stderr = self.orig


    def write(self, data):
        self.orig.write(data)
        add_output(self.color_replace.sub('', data))


    def flush(self):
        self.orig.flush()


g_stdout_log = None
g_stderr_log = None

def start_logging():
    global g_stdout_log, g_stderr_log
    g_stdout_log = Log(1)
    g_stderr_log = Log(2)


def stop_logging():
    global g_stdout_log, g_stderr_log
    g_stdout_log = None
    g_stderr_log = None


def log(s):
    sys.stdout.write("%s %s\n" % (time.strftime("%Y-%m-%d %H:%M:%S"), s))


def hostname():
    return socket.gethostname()


def is_root():
    return os.getuid() == 0


def is_cma():
    return os.path.exists("/etc/cma/cma.conf")


def site_id():
    return os.environ.get("OMD_SITE")


def system_config_path():
    return "/etc/cma/backup.conf"


def site_config_path():
    if not site_id():
        raise Exception("Not executed in OMD environment!")
    return "%s/etc/check_mk/backup.mk" % os.environ["OMD_ROOT"]


# Es gibt ein globales Backup-Lock, das bei modifizierenden Aktionen
# geholt wird. D.h. es kann Systemweit immer nur ein Backup oder Restore
# zur Zeit ausgeführt werden.
def acquire_backup_lock():
    global g_backup_lock_f
    lock_file_path = "%s/mkbackup.lock" % g_var_path
    try:
        g_backup_lock_f = open(lock_file_path, "a+")
    except IOError, e:
        raise MKGeneralException("Failed to open lock file \"%s\": %s" %
                    (lock_file_path, e))

    try:
        fcntl.flock(g_backup_lock_f, fcntl.LOCK_EX | fcntl.LOCK_NB)
    except IOError:
        raise MKGeneralException("Failed to get the exclusive backup lock. "
                            "Another backup/restore seems to be running.")


# TODO: Move to cmklib?
def makedirs(path, user=None, group=None, mode=None):
    head, tail = os.path.split(path)
    if not tail:
        head, tail = os.path.split(head)

    if head and tail and not os.path.exists(head):
        try:
            makedirs(head, user, group, mode)
        except OSError, e:
            # be happy if someone already created the path
            if e.errno != errno.EEXIST:
                raise
        if tail == ".": # xxx/newdir/. exists if xxx/newdir exists
            return
    makedir(path, user, group, mode)


# TODO: Move to cmklib?
def makedir(path, user=None, group=None, mode=None):
    if os.path.exists(path):
        return

    os.mkdir(path)

    if user != None:
        uid = pwd.getpwnam(user).pw_uid
    else:
        uid = -1

    if group != None:
        gid = grp.getgrnam(group).gr_gid
    else:
        gid = -1

    os.chown(path, uid, gid)
    os.chmod(path, mode)


# Wenn als root ausgeführt:
# - System-Konfiguration laden
# Wenn als Site-User ausgeführt:
# - Targets aus System-Konfiguration laden
# - Site-Konfiguration laden
def load_config():
    def load_file(path):
        return eval(file(path).read())

    if is_root():
        config = load_file(system_config_path())
    else:
        config = load_file(site_config_path())

        try:
            system_targets = load_file(system_config_path())["targets"]
            config["targets"].update(system_targets)
        except IOError:
            # Not existing system wide config is OK. In this case there
            # are only backup targets from site config available.
            pass

    return config


def load_backup_info(path):
    return eval(file(path).read())


def save_backup_info():
    files = get_files_for_backup_info()

    info = {
        "type"       : "Check_MK" if not is_root() \
                       else "Appliance",
        "job_id"     : g_local_job_id,
        "compressed" : compress_archives(),
        "encrypted"  : encrypt_archives(),
        "hostname"   : hostname(),
        "files"      : get_files_for_backup_info(),
        "finished"   : time.time(),
        "size"       : sum([ f[1] for f in files ]),
    }

    if not is_root():
        info["site_id"] = site_id()

    with open(backup_info_path(), "w") as f:
        f.write(pprint.pformat(info)+"\n")


def get_files_for_backup_info():
    files = []
    backup_path = job_backup_path_during_backup()
    for f in sorted(os.listdir(backup_path)):
        files.append((f, os.path.getsize(backup_path + "/" + f),
                      file_checksum(backup_path + "/" + f)))

    return files


def file_checksum(path):
    hash_md5 = md5()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


#   List: Alle Backups auflisten
#       Als Site-Nutzer sieht man nur die Site-Backups (auch die, die
#       durch die Systembackups erstellt wurden)
#   - Job-ID
#
#   Beispielbefehle:
#     # listet alle Backups auf die man sehen darf
#     mkbackup list nfs
#
#     # listet alle Backups auf die man sehen darf die zu diesem Job gehören
#     mkbackup list nfs --job=xxx
#
#   Restore:
#   - Job-ID
#   - Backup-ID
#     - Als Site-Nutzer muss man die Backup-ID eines Site-Backups angeben
#
#   Beispielbefehle:
#     # listet alle Backups auf die man sehen darf
#     mkbackup restore nfs backup-id-20
#
#   Show: Zeigt Metainfos zu einem Backup an
#   - Job-ID
#   - Backup-ID
#
#   Beispielbefehle:
#     mkbackup show nfs backup-id-20


modes = {
    "backup": {
        "description":
            "Starts creating a new backup. When executed as Check_MK site user, a backup of the "
            "current site is executed to the target of the given backup job. When executed as "
            "root user on the Check_MK Appliance, a backup of the whole system is created.",
        "args": [
            {
                "id": "Job-ID",
                "description": "The ID of the backup job to work with",
            },
        ],
        "opts": {
            "background": {
                "description": "Fork and execute the program in the background.",
            },
        },
        "root_opts": {
            "without-sites": {
                "description": "Exclude the Check_MK site files during backup.",
            },
        },
    },
    "restore": {
        "description": "Starts the restore of a backup",
        "args": [
            {
                "id": "Target-ID",
                "description": "The ID of the backup target to work with",
            },
            {
                "id": "Backup-ID",
                "description": "The ID of the backup to restore",
            },
        ],
    },
    "jobs": {
        "description": "Lists all configured backup jobs of the current user context.",
    },
    "targets": {
        "description": "Lists all configured backup targets of the current user context.",
    },
    "list": {
        "description": "Output the list of all backups found on the given backup target",
        "args": [
            {
                "id": "Target-ID",
                "description": "The ID of the backup target to work with",
            },
        ],
    },
#    "show": {
#        "description": "Shows information about a backup",
#        "args": [
#            {
#                "id": "Target-ID",
#                "description": "The ID of the backup target to work with",
#            },
#            {
#                "id": "Backup-ID",
#                "description": "The ID of the backup to show",
#            },
#        ],
#    },
}


def mode_backup(local_job_id, opts):
    acquire_backup_lock()

    global g_job_id, g_local_job_id
    g_job_id = globalize_job_id(local_job_id)
    g_local_job_id = local_job_id

    if local_job_id not in g_config["jobs"]:
        raise MKGeneralException("This backup job does not exist.")

    global g_job_config
    g_job_config = g_config["jobs"][local_job_id]

    verify_target_is_ready(g_job_config["target"])

    init_new_run()

    if "background" in opts:
        daemon.daemonize()
        save_state({"pid": os.getpid()})

    start_logging()
    log("--- Starting backup (%s) ---" % g_job_id)

    success = False
    try:
        cleanup_previous_incomplete_backup()

        save_state({
            "state" : "running",
        })

        do_backup(opts)
        success = True

    except KeyboardInterrupt:
        raise

    except:
        if not opt_debug:
            sys.stderr.write("An exception occured:\n")
            sys.stderr.write(traceback.format_exc())
        else:
            raise

    finally:
        stop_logging()
        save_state({
            "state"    : "finished",
            "finished" : time.time(),
            "success"  : success,
        })


def do_backup(opts):
    if not is_root():
        do_site_backup(opts)
    elif is_cma():
        do_system_backup(opts)
    else:
        raise MKGeneralException("System backup not supported.")
    complete_backup()


def do_site_backup(opts, site=None):
    cmd = ["omd", "backup"]

    if not compress_archives():
        cmd.append("--no-compression")

    # When executed as site user, "omd backup" is executed without the site
    # name and always performing backup for the current site. When executed
    # as root, the site argument has to be given and must be handed over to
    # "omd backup".
    if site == None:
        site = site_id()
    else:
        if not is_root():
            raise MKGeneralException("Requested backup of site %s, "
                                     "but not running as root." % site_id)
        cmd.append(site)

    cmd.append("-")

    backup_path = site_backup_path(site)

    # Create missing directories. Ensure group permissions and mode.
    makedirs(os.path.dirname(backup_path), group="omd", mode=0775)

    p = subprocess.Popen(cmd, shell=False, close_fds=True,
                         stdout=subprocess.PIPE, stderr=subprocess.PIPE,
                         stdin=open(os.devnull))

    with open(backup_path, "w") as backup_file:
        for chunk in backup_stream(stream=p.stdout, is_alive=lambda: p.poll() is None):
            backup_file.write(chunk)

    if p.returncode != 0:
        raise MKGeneralException("Site backup failed: %s" % p.stderr.read())


# Using RSA directly to encrypt the whole backup is a bad idea. So we use the RSA
# public key to generate and encrypt a shared secret which is then used to encrypt
# the backup with AES.
#
# When encryption is active, this function uses the configured RSA public key to
# a) create a random secret key which is encrypted with the RSA public key
# b) the encrypted key is used written to the backup file
# c) the unencrypted random key is used as AES key for encrypting the backup stream
def backup_stream(stream, is_alive):
    last_state_update, last_bps, bytes_copied = time.time(), None, 0

    if encrypt_archives():
        secret_key, encrypted_secret_key, iv = derive_key_and_iv(get_encryption_public_key(), 32,
                                                                 AES.block_size)
        cipher = AES.new(secret_key, AES.MODE_CBC, iv)

        # Write out a file version marker and  the encrypted secret key, preceded by 
        # a length indication. All separated by \0.
        yield "%d\0%d\0%s\0" % (1, len(encrypted_secret_key), encrypted_secret_key)
    else:
        pubkey = None

    while True:
        finished = False
        if encrypt_archives():
            chunk = stream.read(1024 * AES.block_size)

            # Detect end of file and add padding to fill up to block size
            if chunk == "" or len(chunk) % AES.block_size != 0:
                padding_length = (AES.block_size - len(chunk) % AES.block_size) or AES.block_size
                chunk += padding_length * chr(padding_length)
                finished = True
        else:
            chunk = stream.read(1024 * 1024)

            if chunk == "":
                finished = True

        bytes_copied += len(chunk)

        if encrypt_archives():
            yield cipher.encrypt(chunk)
        else:
            yield chunk

        if finished and not is_alive():
            break # end of tar archive reached

        timedif = time.time() - last_state_update
        if timedif >= 1:
            this_bps = float(bytes_copied) / timedif

            if last_bps == None:
                bps = this_bps # initialize the value
            else:
                percentile, backlog_sec = 0.50, 10
                weight_per_sec = (1 - percentile) ** (1.0 / backlog_sec)
                weight = weight_per_sec ** timedif
                bps = last_bps * weight + this_bps * (1 - weight)

            save_state({"bytes_per_second": bps})
            last_state_update, last_bps, bytes_copied = time.time(), bps, 0


# logic from http://stackoverflow.com/questions/6309958/encrypting-a-file-with-rsa-in-python
def derive_key_and_iv(pubkey, key_length, iv_length):
    secret_key = os.urandom(key_length)

    # Padding (see explanations below)
    plaintext_length = (Crypto.Util.number.size(pubkey.n) - 2) / 8
    padding = '\xff' + os.urandom(16)
    padding += '\0' * (plaintext_length - len(padding) - len(secret_key))

    # Encrypt the secret key with the RSA public key
    encrypted_secret_key = pubkey.encrypt(padding + secret_key, None)[0]

    # The iv is an initialization vector for the CBC mode of operation. It
    # needs to be unique per key per message. Normally, it's sent alongside
    # the data in cleartext. Here, since the key is only ever used once,
    # you can use a known IV.
    iv = '\x00' * iv_length

    return secret_key, encrypted_secret_key, iv


#def decrypt(in_file, out_file, password, key_length=32):
#    bs = AES.block_size
#    key, iv = derive_key_and_iv(password, key_length, bs)
#    cipher = AES.new(key, AES.MODE_CBC, iv)
#    next_chunk = ''
#    finished = False
#    while not finished:
#        chunk, next_chunk = next_chunk, cipher.decrypt(in_file.read(1024 * bs))
#        if len(next_chunk) == 0:
#            padding_length = ord(chunk[-1])
#            chunk = chunk[:-padding_length]
#            finished = True
#        out_file.write(chunk)


def get_encryption_public_key():
    key_id = g_job_config["encrypt"]
    keys = load_backup_keys()

    try:
        key = keys[key_id]
    except KeyError:
        raise MKGeneralException("Failed to load the configured backup key: %d" % key_id)

    # First extract the public key part from the certificate
    cert = crypto.load_certificate(crypto.FILETYPE_PEM, key["certificate"])
    pub  = cert.get_pubkey()
    pub_pem = crypto.dump_publickey(crypto.FILETYPE_PEM, pub)

    # Now constuct the public key object
    return RSA.importKey(pub_pem)


def load_backup_keys():
    if is_root():
        path = "/etc/cma/backup_keys.conf"
    else:
        path = "%s/etc/check_mk/backup_keys.mk" % os.environ["OMD_ROOT"]

    variables = { "keys" : {} }
    execfile(path, variables, variables)
    return variables["keys"]


def backup_info_path():
    return "%s/mkbackup.info" % (job_backup_path_during_backup())


def site_backup_path(site_id):
    return "%s/site-%s%s" % (job_backup_path_during_backup(), site_id, archive_suffix())


def system_backup_path():
    return "%s/system" % (job_backup_path_during_backup(), archive_suffix())


def system_data_backup_path():
    return "%s/system-data%s" % (job_backup_path_during_backup(), archive_suffix())


def job_backup_path_during_backup():
    return "%s-incomplete" % job_backup_path()


def job_backup_path_complete():
    return "%s-complete" % job_backup_path()


def job_backup_path():
    return "%s/%s" % (target_path(g_job_config["target"]), g_job_id)


def archive_suffix():
    suffix = ".tar"
    if compress_archives():
        suffix += ".gz"
    if encrypt_archives():
        suffix += ".enc"
    return suffix


def compress_archives():
    return "compress" in g_job_config


def encrypt_archives():
    return "encrypt" in g_job_config


def target_cfg(target_ident):
    return g_config["targets"][target_ident]


def target_path(target_ident):
    cfg = target_cfg(target_ident)
    if cfg["remote"][0] != "local":
        raise NotImplementedError()

    return cfg["remote"][1]["path"]


def verify_target_is_ready(target_ident):
    cfg = target_cfg(target_ident)
    if cfg["remote"][0] != "local":
        raise NotImplementedError()

    if cfg["remote"][1]["is_mountpoint"] and not os.path.ismount(cfg["remote"][1]["path"]):
        raise MKGeneralException("The backup target path is configured to be a mountpoint, "
                                 "but nothing is mounted.")


def do_system_backup(opts):
    # Create missing directories. Ensure group permissions and mode.
    makedirs(os.path.dirname(system_backup_path()), group="omd", mode=0775)

    # Perform backup of the /rw volume on all devices
    log("Performing system backup (system%s)" % archive_suffix())
    do_system_rw_backup(opts)

    # The data volume (/omd) is not backed up on slave cluster nodes
    if not is_cluster_inactive():
        log("Performing system data backup (system-data%s)" % archive_suffix())
        do_system_data_backup(opts)
    else:
        log("Skipping system data backup (inactive cluster node)")

    def exclude_sites(opts):
        return "without-sites" in opts \
               or g_job_config.get("without_sites", False) == True

    # Now run the site backup for all sites
    if not exclude_sites(opts):
        for site_id in existing_sites():
            log("Performing site backup: %s" % site_id)
            do_site_backup(opts, site=site_id)
    else:
        log("Skipping site backup (disabled)")


def do_system_rw_backup(opts):
    with open(system_backup_path(), "w") as backup_file:
        pipein_fd, pipeout_fd = os.pipe()
        pipein  = os.fdopen(pipein_fd)

        # Write to buffer in dedicated thread
        t = threading.Thread(target=lambda: write_to_tarfile_threaded(
                                                pipeout_fd, "/rw", ["mnt/*/*"]))
        t.daemon = True
        t.start()

        # Process backup stream and write to destination file
        for chunk in backup_stream(stream=pipein, is_alive=lambda: t.is_alive()):
            backup_file.write(chunk)


def do_system_data_backup(opts):
    with file(system_data_backup_path(), "w") as backup_file:
        pipein_fd, pipeout_fd = os.pipe()
        pipein  = os.fdopen(pipein_fd)

        # Write to buffer in dedicated thread
        t = threading.Thread(target=lambda: write_to_tarfile_threaded(
                                                pipeout_fd, "/opt/omd/", ["sites/*"]))
        t.daemon = True
        t.start()

        # Process backup stream and write to destination file
        for chunk in backup_stream(stream=pipein, is_alive=lambda: t.is_alive()):
            backup_file.write(chunk)


def write_to_tarfile_threaded(pipeout_fd, base_path, exclude_patterns):
    pipeout = os.fdopen(pipeout_fd, "w")
    backup_files_to_tarfile(pipeout, base_path, exclude_patterns)
    pipeout.close()


def is_cluster_inactive():
    return False # TODO


def existing_sites():
    return sorted([ s for s in os.listdir("/omd/sites")
                    if os.path.isdir(os.path.join("/omd/sites/", s)) ])


def backup_files_to_tarfile(fobj, base_path, exclude_patterns=None):
    if exclude_patterns:
        def filter_files(filename):
            for glob_pattern in exclude_patterns:
                # patterns are relative to base_path, filename is full path.
                # strip of the base_path prefix from full path
                if fnmatch.fnmatch(filename[len(base_path.rstrip("/"))+1:], glob_pattern):
                    return True # exclude this file
            return False

    else:
        filter_files = lambda x: False

    tar_mode = "w:gz" if compress_archives() else "w:"
    tar = TarFile.open(fileobj=fobj, mode=tar_mode)
    tar.add(base_path, exclude=filter_files)
    tar.close()


def complete_backup():
    if os.path.exists(job_backup_path_complete()):
        log("Cleaning up previously completed backup")
        shutil.rmtree(job_backup_path_complete())

    save_backup_info()

    os.rename(job_backup_path_during_backup(),
              job_backup_path_complete())

    duration = time.time() - load_state()["started"]
    log("--- Backup completed (%0.2f sec) ---" % duration)


def cleanup_previous_incomplete_backup():
    if os.path.exists(job_backup_path_during_backup()):
        log("Found previous incomplete backup. Cleaning up those files.")
        shutil.rmtree(job_backup_path_during_backup())


def globalize_job_id(local_job_id):
    parts = []
    site = site_id()

    if site:
        parts.append("Check_MK")
    else:
        parts.append("Check_MK_Appliance")

    parts.append(hostname())

    if site:
        parts.append(site)

    parts.append(local_job_id)

    return "-".join([ p.replace("-", "+") for p in parts ])


def init_new_run():
    save_state({
        "state"            : "started",
        "pid"              : os.getpid(),
        "started"          : time.time(),
        "output"           : "",
        "bytes_per_second" : 0,
    }, update=False)


def load_state():
    return eval(file(state_path(g_job_id)).read())


def save_state(new_attrs, update=True):
    if update:
        state = load_state()
    else:
        state = {}
    state.update(new_attrs)

    with open(state_path(g_job_id), "w") as f:
        f.write(pprint.pformat(state)+"\n")


def state_path(job_id):
    return "%s/%s.state" % (g_var_path, job_id)


def add_output(s):
    state = load_state()
    state["output"] += s
    save_state(state, update=False)


# TODO: Implement this
def mode_restore(target_id, backup_id, opts):
    acquire_backup_lock()

    print target_id, backup_id, opts


def mode_list(target_id, opts):
    if target_id not in g_config["targets"]:
        raise MKGeneralException("This backup target does not exist. Choose one of: %s" %
                                    ", ".join(g_config["targets"].keys()))

    fmt = "%-9s %-20s %-20s %28s\n"
    fmt_detail = (" " * 30) + " %-20s %28s\n"
    sys.stdout.write(fmt % ("Type", "Job", "Details", ""))
    sys.stdout.write("%s\n" % ("-" * 80))
    for path in sorted(glob.glob("%s/*/mkbackup.info" % target_path(target_id))):
        info = load_backup_info(path)
        from_info = info["hostname"]
        if "site_id" in info:
            from_info += " (Site: %s)" % info["site_id"]
        sys.stdout.write(fmt % (info["type"], info["job_id"], "From:", from_info))

        sys.stdout.write(fmt_detail % ("Finished:", render.date_and_time(info["finished"])))
        sys.stdout.write(fmt_detail % ("Size:", render.bytes(info["size"])))
    sys.stdout.write("\n")


def mode_jobs(opts):
    fmt = "%-29s %-30s\n"
    sys.stdout.write(fmt % ("Job-ID", "Title"))
    sys.stdout.write("%s\n" % ("-" * 60))
    for job_id, job_cfg in sorted(g_config["jobs"].items(), key=lambda (x, y): x):
        sys.stdout.write(fmt % (job_id, job_cfg["title"]))


def mode_targets(opts):
    fmt = "%-29s %-30s\n"
    sys.stdout.write(fmt % ("Target-ID", "Title"))
    sys.stdout.write("%s\n" % ("-" * 60))
    for job_id, job_cfg in sorted(g_config["targets"].items(), key=lambda (x, y): x):
        sys.stdout.write(fmt % (job_id, job_cfg["title"]))


def usage(error=None):
    if error:
        sys.stderr.write("ERROR: %s\n" % error)
    sys.stdout.write("Usage: mkbackup [OPTIONS] MODE [MODE_ARGUMENTS...] [MODE_OPTIONS...]\n")
    sys.stdout.write("\n")
    sys.stdout.write("OPTIONS:\n")
    sys.stdout.write("\n")
    sys.stdout.write("    --verbose     Enable verbose output, twice for more details\n")
    sys.stdout.write("    --debug       Let Python exceptions come through\n")
    sys.stdout.write("    --version     Print the version of the program\n")
    sys.stdout.write("\n")
    sys.stdout.write("MODES:\n")
    sys.stdout.write("\n")

    for mode_name, mode in sorted(modes.items()):
        mode_indent = " " * 18
        wrapped_descr = textwrap.fill(mode["description"], width=82,
                                      initial_indent="    %-13s " % mode_name,
                                      subsequent_indent=mode_indent)
        sys.stdout.write(wrapped_descr + "\n")
        sys.stdout.write("\n")
        if "args" in mode:
            sys.stdout.write("%sMODE ARGUMENTS:\n" % mode_indent)
            sys.stdout.write("\n")
            for arg in mode["args"]:
                sys.stdout.write("%s  %-10s %s\n" % (mode_indent, arg["id"], arg["description"]))
            sys.stdout.write("\n")

        opts = mode_options(mode)
        if opts:
            sys.stdout.write("%sMODE OPTIONS:\n" % mode_indent)
            sys.stdout.write("\n")

            for opt_id, opt in sorted(opts.items(), key=lambda (k, v): k):
                sys.stdout.write("%s  --%-13s %s\n" % (mode_indent, opt_id, opt["description"]))
            sys.stdout.write("\n")

    sys.stdout.write("\n")
    sys.exit(3)


def mode_options(mode):
    opts = {}
    opts.update(mode.get("opts", {}))
    if is_root():
        opts.update(mode.get("root_opts", {}))
    return opts


def interrupt_handler(signum, frame):
    raise MKTerminate("Caught signal: %d" % signum)


def register_signal_handlers():
    signal.signal(signal.SIGTERM, interrupt_handler)

g_config    = {}
opt_verbose = 0
opt_debug   = False


def main():
    global opt_debug, opt_verbose, g_config

    register_signal_handlers()

    short_options = "h"
    # TODO: Implement verbose handling
    long_options = [ "help", "version", "verbose", "debug" ]

    opts, args = getopt.getopt(sys.argv[1:], short_options, long_options)

    for o, a in opts:
        if o in [ "-h", "--help" ]:
            usage()
        elif o == "--version":
            sys.stdout.write("mkbackup %s\n" % VERSION)
            sys.exit(0)
        elif o == "--verbose":
            opt_verbose += 1
        elif o == "--debug":
            opt_debug = True

    try:
        mode_name = args.pop(0)
    except IndexError:
        usage("Missing operation mode")

    try:
        mode = modes[mode_name]
    except KeyError:
        usage("Invalid operation mode")

    try:
        g_config = load_config()
    except IOError:
        if opt_debug:
            raise
        raise MKGeneralException("mkbackup is not configured yet.")

    # Load the mode specific options
    mode_opts, mode_args = getopt.getopt(sys.argv[2:], [], mode_options(mode).keys())

    # Validate arguments
    if len(mode_args) != len(mode.get("args", [])):
        usage("Invalid number of arguments for this mode")

    opt_dict = dict([ (k.lstrip("-"), v) for k, v in opts + mode_opts ])

    globals()["mode_%s" % mode_name](*mode_args, opts=opt_dict)


try:
    main()
except MKTerminate, e:
    sys.stderr.write("%s\n" % e)
    sys.exit(1)

except KeyboardInterrupt:
    sys.stderr.write("Terminated.\n")
    sys.exit(0)

except MKGeneralException, e:
    sys.stderr.write("%s\n" % e)
    if opt_debug:
        raise
    sys.exit(3)
